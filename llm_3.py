"""
Agente LangGraph con Telegram y LangSmith
==========================================
Este ejercicio crea un agente conversacional que:
1. Responde consultas por Telegram
2. Usa Gemini 2.5-flash como LLM
3. Es observado por LangSmith para trazabilidad
"""

import os
from dotenv import load_dotenv
from typing import TypedDict, Annotated, Sequence
from operator import add

# LangChain y LangGraph
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# Telegram
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

# Cargar variables de entorno
load_dotenv()

# ==========================================
# CONFIGURACI√ìN DE LANGSMITH
# ==========================================
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_PROJECT"] = "telegram-agent-langgraph"
# LANGCHAIN_API_KEY se carga autom√°ticamente del .env

# ==========================================
# DEFINICI√ìN DEL ESTADO DEL AGENTE
# ==========================================
class AgentState(TypedDict):
    """Estado del agente que mantiene el historial de mensajes"""
    messages: Annotated[Sequence[BaseMessage], add]
    user_id: str

# ==========================================
# CONFIGURACI√ìN DEL MODELO LLM
# ==========================================
llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    max_tokens=1024
)

# ==========================================
# NODOS DEL GRAFO
# ==========================================

def process_message(state: AgentState) -> dict:
    """
    Nodo principal que procesa el mensaje del usuario.
    A√±ade contexto del sistema y genera la respuesta.
    """
    messages = state["messages"]
    
    # Mensaje del sistema con instrucciones para el agente
    system_message = SystemMessage(content="""
    Eres un asistente virtual inteligente y amigable que responde consultas por Telegram.
    
    Instrucciones:
    - Responde de manera clara, concisa y √∫til
    - Usa emojis ocasionalmente para hacer la conversaci√≥n m√°s amigable
    - Si no sabes algo, adm√≠telo honestamente
    - Puedes responder en espa√±ol o ingl√©s seg√∫n el idioma del usuario
    - Mant√©n un tono profesional pero cercano
    """)
    
    # Construir la lista de mensajes para el LLM
    full_messages = [system_message] + list(messages)
    
    # Invocar el modelo
    response = llm.invoke(full_messages)
    
    return {"messages": [response]}


def should_continue(state: AgentState) -> str:
    """
    Nodo de decisi√≥n: determina si continuar o terminar.
    Por ahora siempre termina despu√©s de una respuesta.
    """
    return END

# ==========================================
# CONSTRUCCI√ìN DEL GRAFO CON LANGGRAPH
# ==========================================

def create_agent_graph():
    """Crea y compila el grafo del agente."""
    
    # Crear el grafo de estados
    workflow = StateGraph(AgentState)
    
    # A√±adir nodos
    workflow.add_node("process", process_message)
    
    # Definir el punto de entrada
    workflow.set_entry_point("process")
    
    # A√±adir aristas
    workflow.add_edge("process", END)
    
    # Compilar con memoria para mantener el historial por usuario
    memory = MemorySaver()
    graph = workflow.compile(checkpointer=memory)
    
    return graph

# Crear la instancia del agente
agent = create_agent_graph()

# ==========================================
# HANDLERS DE TELEGRAM
# ==========================================

async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Comando /start - Mensaje de bienvenida"""
    welcome_message = """
üëã ¬°Hola! Soy un agente inteligente powered by LangGraph y Gemini.

Puedo ayudarte con:
‚Ä¢ Responder preguntas generales
‚Ä¢ Mantener conversaciones contextuales
‚Ä¢ Asistirte con diversas consultas

üí° Simplemente escr√≠beme tu pregunta y te responder√©.

Comandos disponibles:
/start - Mostrar este mensaje
/clear - Limpiar el historial de conversaci√≥n
/help - Obtener ayuda
    """
    await update.message.reply_text(welcome_message)


async def clear_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Comando /clear - Limpia el historial de conversaci√≥n"""
    user_id = str(update.effective_user.id)
    context.user_data.clear()
    await update.message.reply_text("üóëÔ∏è Historial de conversaci√≥n limpiado. ¬°Empecemos de nuevo!")


async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Comando /help - Muestra ayuda"""
    help_text = """
üìö **Gu√≠a de uso**

Este bot utiliza LangGraph para mantener conversaciones inteligentes.

**Caracter√≠sticas:**
- Memoria contextual por usuario
- Respuestas generadas por Gemini 2.5-flash
- Trazabilidad completa en LangSmith

**Tips:**
- Haz preguntas claras y espec√≠ficas
- Usa /clear si quieres empezar una conversaci√≥n nueva
- El bot recuerda el contexto de la conversaci√≥n
    """
    await update.message.reply_text(help_text, parse_mode='Markdown')


async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """
    Maneja los mensajes de texto del usuario.
    Invoca el agente LangGraph y devuelve la respuesta.
    """
    user_id = str(update.effective_user.id)
    user_message = update.message.text
    
    # Mostrar indicador de escritura
    await context.bot.send_chat_action(
        chat_id=update.effective_chat.id,
        action="typing"
    )
    
    try:
        # Configuraci√≥n del thread para mantener memoria por usuario
        config = {"configurable": {"thread_id": user_id}}
        
        # Crear el mensaje de entrada
        input_state = {
            "messages": [HumanMessage(content=user_message)],
            "user_id": user_id
        }
        
        # Invocar el agente
        result = agent.invoke(input_state, config=config)
        
        # Obtener la √∫ltima respuesta del agente
        ai_response = result["messages"][-1].content
        
        # Enviar respuesta al usuario
        await update.message.reply_text(ai_response)
        
    except Exception as e:
        error_message = f"‚ùå Ocurri√≥ un error: {str(e)}"
        print(f"Error procesando mensaje: {e}")
        await update.message.reply_text(error_message)


async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Maneja errores globales del bot"""
    print(f"Error: {context.error}")


# ==========================================
# FUNCI√ìN PRINCIPAL
# ==========================================

def main():
    """Inicializa y ejecuta el bot de Telegram."""
    
    # Obtener el token del bot
    telegram_token = os.getenv("TELEGRAM_BOT_TOKEN")
    
    if not telegram_token:
        print("‚ùå Error: TELEGRAM_BOT_TOKEN no est√° configurado en el archivo .env")
        print("Por favor, a√±ade tu token de Telegram al archivo .env:")
        print("TELEGRAM_BOT_TOKEN=tu_token_aqu√≠")
        return
    
    print("üöÄ Iniciando bot de Telegram con LangGraph...")
    print("üìä LangSmith est√° configurado para observabilidad")
    print("üß† Usando Gemini 2.5-flash como modelo")
    
    # Crear la aplicaci√≥n
    application = Application.builder().token(telegram_token).build()
    
    # A√±adir handlers
    application.add_handler(CommandHandler("start", start_command))
    application.add_handler(CommandHandler("clear", clear_command))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    
    # Handler de errores
    application.add_error_handler(error_handler)
    
    # Ejecutar el bot
    print("‚úÖ Bot iniciado. Presiona Ctrl+C para detener.")
    application.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    main()
